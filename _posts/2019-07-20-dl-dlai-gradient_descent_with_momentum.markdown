---
layout: post
title: Gradient descent with momentum
date: 2019-07-20 01:00:00
img: dl/dlai/dlai.png
categories: [dl-dlai] 
tags: [python, deep learning, optimization, gradient descent, momentum] # add tag
---

- 이전 글 : [Bias Correction of Exponentially Weighted ](https://gaussian37.github.io/dl-dlai-bias_correction_exponentially_weighted_averages/)

<br>
<div style="text-align: center;">
    <iframe src="https://www.youtube.com/embed/k8fTYJPd3_I" frameborder="0" allowfullscreen="true" width="600px" height="400px"> </iframe>
</div>
<br>

- 이번 글에서는 앞에서 배운 지수 가중 평균을 이용하여 그래디언트 디센트 방법 보다 조금 더 개선된 `모멘텀` 방법에 대하여 배워보겠습니다.
- 모멘텀 알고리즘은 기본적인 그래디언트 디센트 방법을 발전한 방법으로 거의 항상 더 빠르게 학습을 할 수 있다고 생각하셔도 됩니다.
- 기본적인 아이디어는 경사에 대한 지수가중평균을 계산하는 것이고 그 값으로 가중치를 업데이트 하는 방식입니다.

<center><img src="../assets/img/dl/dlai/gradient_descent_with_momentum/1.png" alt="Drawing" style="width: 800px;"/></center>

<br>

- 비용 함수를 최적화 한다고 가정하면 위 슬라이드의 빨간색 점은 우리가 목표로 하는 최솟값이라고 할 수 있습니다.
    - 즉 등고선의 바깥쪽이 비용이 큰 상태이고 등고선의 안쪽이 비용이 작은 상태입니다.
- 일반적으로 미니 배치 그래디언트 디센트 또는 배치 그래디언트 디센트 방법을 사용하면 2차원 이상의 그래프에서 보았을 때, 비용의 최소값 까지 단거리로 도달 하지 않고 **진동**을 하면서 도달하게 됩니다. (파란색 선)
- 그러다가 많은 단계를 지나고 나면 최솟값으로 가면서 천천히 진동을 하게 됩니다.
- 슬라이드와 같이 위 아래로 진동하게 되면 그래디언트 디센트의 속도를 느리게 하고 더 큰 러닝 레이트를 사용하는 것을 어렵게 만듭니다.
    - 보라색 선과 같이 큰 러닝 레이트를 사용하면 오버 슈팅하게 되어 위와 같은 식으로 발산할 수도 있기 때문입니다.

<br>

- 이 문제를 보는 또 다른 관점은 수직축에서는 진동을 막기 위해 학습이 더 느리게 일어나기를 원하지만 수평축에서는 더 빠른 학습을 원한다고 생각할 수 있습니다.
- 결과적으로는 최솟값을 향해 왼쪽에서 오른쪽으로 이동하는 것을 처리하고 싶기 때문에 수직축의 진동을 막으면 왼쪽에서 오른쪽으로 더 빨리 도달할 수 있을 것입니다.
- 그러면 이 방법을 구현하기 위한 모멘텀 방법을 본격적으로 알아보도록 하겠습니다.

<br>

- 모멘텀을 구현하기 위해서는 각 iteration에서 각 배치(또는 미니 배치)에 도함수인 $$ dW, db $$를 계산해야 합니다. 여기 까지는 일반적인 그래디언트 디센트 방법과 같습니다.
- 그 다음으로 $$ v_{dw} = \beta * v_{dw} + (1-\beta)*dw $$를 계산합니다. 이 식의 형태는 앞의 글에서 배운 지수 가중 평균인 $$ v_{\theta} = \beta * v_{\theta} + (1-\beta)*\theta $$ 형태와 같습니다.
    - $$ v_{dw} $$ 값을 구할 때에 사용되는 이동 평균은 $$ w $$에 대한 도함수($$ dw $$)가 됩니다.
- 그리고 bias에 대하여도 동일하게 $$ v_{db} = \beta * v_{db} + (1-\beta)*db $$ 지수가중평균을 적용할 수 있습니다.
- 그리고 나서 이 값을 이용하여 weight와 bias를 업데이트 할 수 있습니다.
- 즉, $$ w = w - \alpha * v_{dw}, b = b - \alpha * v_{db} $$ 로 업데이트를 합니다.
- 일반적인 그래디언트 디센트와 달라진 것은 그래디언트 디센트에서는 단순히 러닝 레이트와 그래디언트(도함수 값)를 곱한 값을 이용하여 weight와 bias를 업데이트 하였지만 모멘텀에서는 단순한 그래디언트(도함수 값) 대신에 지수가중평균 처리를 한 값을 사용한다는 점입니다. 
- 위 슬라이드의 파란색 선과 같이 수직으로 진동하는 형태에서는 위로 향하는 크기와 아래로 향하는 크기의 평균을 구하면 아주 약한 진동을 하는 값이 될 것입니다.
    - 모멘텀에서 가한 연산이 지수 가중 `평균`이기 때문에 수직 방향과 같이 +, - 크기로 진동을 하는 값은 평균이 0에 가깝게 되어 진동 폭이 줄어들게 됩니다.
    - 반면 수평 축에서는 모든 도함수가 오른쪽 방향을 가리키고 있기 때문에 수평 방향의 평균은 꽤 큰 값을 가지게 됩니다. 
- 따라서 모멘텀 방법을 적용하게 되면 슬라이드의 빨간색 선과 같이 진동이 줄어든 상태에서 최소값에 도달할 수 있게 됩니다. 
- 또한 수평 방향에서는 더 빠르게 움직이게 되므로 전체적으로 학습 속도는 더 빨라지게 됩니다. 

<br>

- 모멘텀 식을 좀 더 물리적인 관점에서 바라본다면 등고선 형태는 밥그릇 같은 모양이 될 수 있고 빨간 점은 밥그릇의 가장 아래 가운데 부분이 될 것입니다.
- 이 때, $$ v_{dw}, v_{db} $$를 공이 밥그릇 아래로 내려가는 **속도**라고 본다면 식의 $$ dw, db $$는 밥그릇 아래로 이동하는 속도의 변화율인 **가속도**로 해석할 수 있습니다.
- 그러면 $$ \beta $$ 값은 0과 1사이 값이므로 속도를 줄어들게 하는 **마찰력**이라고 볼 수 있습니다.  

<center><img src="../assets/img/dl/dlai/gradient_descent_with_momentum/2.png" alt="Drawing" style="width: 800px;"/></center>

<br>

- 그러면 모멘텀을 구현하는 방법에 대하여 알아보도록 하겠습니다.
- 여기서는 러닝 레이트 $$ \alpha $$와 지수가중평균에 사용되는 상수 $$ \beta $$가 있습니다. 이 두 값은 하이퍼파라미터로 학습하기 전에 미리 정해주어야 합니다.
- 일반적으로 $$ \beta $$ 값은 0.9를 사용합니다. 경험적으로 잘 되는 값이기에 사용한다고 볼 수 있습니다. 이전 글에서 배운 바와 같이 $$ \beta $$가 0.9이면 이전 10개의 iteration의 평균값이라고 생각 할 수 있습니다. 
- 만약 bias correction을 사용하려면 $$ v_{dw} $$에 $$ 1 - \beta^{t} $$를 나누어 줘야 합니다. 하지만 iteration이 지속되면 효과가 없어지기 때문에 잘 사용하지는 않습니다.
- 논문에 따라서는 $$ (1-\beta) $$가 없는 식으로 다루어 지기도 합니다. 즉, $$ v_{dw} = \beta*v_{dw} + dw $$ 로 사용되기도 하는데, 이런 경우에는 러닝 레이트 $$ \alpha $$에 $$ 1/(1-\beta) $$ 값을 반영한 경우라고 봐야 합니다.
    - 물론 $$ (1-\beta) $$ 항을 적어주는 것이 지수가중평균에 대한 개념도 더 직관적으로 반영이 되어있으므로 $$ (1-\beta) $$ 항을 적어주는 형태를 사용하길 권장합니다.
    
<br>

- 정리하면 모멘텀은 그래디언트 디센트에 지수가중평균 개념을 도입하여 어떤 축으로 진동하는 값을 줄여주어서 학습 속도를 빠르게 하고 빠르게 이동해야 하는 축으로는 좀 더 빠르게 이동할 수 있도록 만들어 주는 방법이라고 볼 수 있습니다.
- 따라서 모멘텀은 거의 항상 그래디언트 디센트 보다 더 동작하게 됩니다.
- 물론 모멘텀 방법 이외에도 더 개선된 방법이 있습니다. 다음 글에서는 RMSProp 방법에 대하여 배워보도록 하겠습니다. 

<br>

- 다음 글 : [RMSProp](https://gaussian37.github.io/dl-dlai-RMSProp/)