---
layout: post
title: Activation function 요약
date: 2019-12-01 00:00:00
img: dl/concept/prelu/prelu.png
categories: [dl-concept] 
tags: [딥러닝, PReLU, parametric ReLU] # add tag
---

<br>

- 이번 글에서는 다양한 Activation function들을 쉽게 요약해 놓은 그림을 이용하여 설명드리겠습니다.

<br>
<center><img src="../assets/img/dl/concept/activation_functions/activation_functions.jpg" alt="Drawing" style="width: 800px;"/></center>
<br>

- 위 그림에서는 그래프와 식 그리고 미분을 했을 때의 gradient에 해당하는 값 까지 보기 쉽게 정리되어 있습니다.
- 특히 자주 사용하는 `ReLU`와 `PReLU`의 식과 미분했을 때의 값을 유심히 보면 되겠습니다.
