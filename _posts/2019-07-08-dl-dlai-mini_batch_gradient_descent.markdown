---
layout: post
title: Mini-batch gradient descent
date: 2019-07-08 00:00:00
img: dl/dlai/dlai.png
categories: [dl-dlai] 
tags: [python, deep learning, optimization, mini-batch gradient descent] # add tag
---

- 이번 시간에는 신경망 네트워크를 훨씬 더 빨리 트레이닝 시킬 수 있도록 도와주는 최적화 알고리즘에 대하여 배워보도록 하겠습니다.
- ML을 적용하는 것은 매우 경험적이고 반복적인 작업을 동반하는 것으로 여러 모델들을 학습시키고 가장 잘 동작하는 것을 찾아야 합니다.
- 따라서 학습의 속도 또한 매우 중요 합니다. 
- 또한 DL의 경우 큰 데이터에 잘 작동하는 경향이 있습니다. 신경망을 큰 데이터 셋에서 학습을 시킬 수 있는데, 이런 경우 학습이 매우 느립니다.
- 이런 경우 빠른 최적화 알고리즘과 양질의 최적화 알고리즘을 갖는 것이 효율성을 높일 수 있습니다. 
- 이번 시간에는 이러한 방법 중 하나인 `미니 배치 그래디언트 디센트`에 대하여 알아보도록 하겠습니다.

<br>

<center><img src="../assets/img/dl/dlai/mini_batch_gradient_descent/1.PNG" alt="Drawing" style="width: 600px;"/></center>

- 먼저 위 슬라이드와 같이 $$ X $$ (학습 데이터) 와 $$ Y $$ (정답 데이터)가 있다고 가정해 보겠습니다.
- 이 때, $$ X $$ 의 차원은 $$ (n_{x}, m) $$이 되고 $$ Y $$의 차원은 $$(1, m)$$이 됩니다. 
- 예를 들어 데이터 셋의 크기인 $$ m $$이 5백만 이라고 가정해 보겠습니다. 꽤 큰 값이지요?
- 이런 경우 한번에 학습할 데이터의 양을 작게 나눈다고 생각해 보겠습니다. 그리고 작은 단위의 학습 데이터셋을 `미니 배치`라고 부르겠습니다.
- 예를 들어 1,000개의 단위로 학습 데이터를 쪼개서 생각한다면 $$ (x^{(1)}, x^{(2)}, \cdots , x^{(1000)}) $$ 까지가 첫번째 미니 배치에 해당합니다.
    - 표기의 편의상 $$ (x^{(1)}, \cdots,  x^{(1000)}) $$ 을 $$ X^{\{1\}} $$로 $$ (x^{(1001)}, \cdots,  x^{(2000)}) $$ 을 $$ X^{\{2\}} $$로 표시하겠습니다.
    - 그러면 마지막 배치는 $$ X^{\{5000\}} $$이 됩니다. 
- 이에 상응하게 $$ Y $$ 데이터도 동일한 크기로 나누어야 합니다. 따라서 Y 데이터도 미니 배치 단위로 $$ Y^{\{1\}}, Y^{\{2\}}, \cdots, Y^{\{5000\}} $$이 됩니다.
- 그러면 한번에 모든 데이터를 이용하여 학습을 하는 경우와 미니 배치 단위로 학습을 하는 경우가 나뉘게 됩니다.
- 앞에서 배워왔던 gradient descent 방법은 한 번에 모든 데이터를 이용하여 학습을 합니다. 즉, 한 번에 학습하는 학습 데이터의 범위가 $$ X, Y $$가 됩니다.
- 반면 이번에 배울 mini-batch gradient descent 방법은 한 번에 미니 배치 하나를 이용하여 학습을 합니다. 즉, 한 번에 학습하는 학습 데이터의 범위는 $$ X^{(t)}. Y^{(t)} $$ 가 됩니다.

<br>

<center><img src="../assets/img/dl/dlai/mini_batch_gradient_descent/2.PNG" alt="Drawing" style="width: 600px;"/></center>


