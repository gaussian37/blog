---
layout: post
title: MLE(Maximum Likelehood Estimation)와 MAP(Maxim A Posterior)에 관하여
date: 2019-09-16 00:00:00
img: ml/concept/machineLearning.jpg
categories: [gan-concept] 
tags: [MLE, ML, Maximum likelihood estimation] # add tag
---

### **Reference**

<br>

- http://sanghyukchun.github.io/58/
- http://rstudio-pubs-static.s3.amazonaws.com/204928_c2d6c62565b74a4987e935f756badfba.html
- http://arkainoh.blogspot.com/2017/10/parametric.learning.maximum.likelihood.estimation.html

<br>

- 이번 글에서는 GAN을 시작하기에 앞서서 필수적인 통계적 개념을 다루려고 합니다.
- 통계학 및 머신러닝 전반적으로 아주 중요한 개념인 MLE(Maximum Likelihood Estimation)와 MAP(Maximum A Posterior)에 대하여 다루어 보겠습니다.
- 제 블로그의 [다른 글](https://gaussian37.github.io/ml-concept-mle-and-map/)을 참조하셔도 충분합니다. 

<br>

### **확률밀도함수(PDF: Probability Density Function)**

<br>

- 먼저 확률밀도함수의 정의를 알기 위하여 아래 두가지 예제를 통하여 `확률`에 대하여 먼저 알아보도록 하겠습니다.
- 주사위를 예를 들어 보겠습니다. 주사위를 던져서 나올 수 있는 숫자는 1, 2, 3, 4, 5, 6이고 각 숫자가 나올 확률은 1/6 으로 모두 같습니다.
- 또 다른 예로 동전을 10번 던져서 앞면은 0 ~ 10번 나올 수 있으며 각각의 확률을 계산해 보면 각각 0.001, 0.01, 0.044, 0.117, 0.205, 0.246, 0.205, 0.117, 0.044, 0.01, 0.001 인 경우를 생각해 보겠습니다.
- 두 경우 모두 일어날 수 있는 사건이 6개, 11개로 정해져 있으며 각각에 대한 확률을 구할 수 있고 확률의 합은 1이 됩니다.

<br>
<center><img src="../assets/img/gan/concept/mle_and_map/1.png" alt="Drawing" style="width: 800px;"/></center>
<br>

- 이번에는 1에서 6 사이의 숫자 중 랜덤으로 아무 숫자나 뽑는다고 가정해 보겠습니다.
- 이 때 정확히 5가 뽑힐 확률은 얼마일까? 1과 6사이에는 무한개의 숫자가 있으니 정확히 5가 뽑힐 확률은 1/∞=0 입니다.
    - 이렇게 `연속적인 구간`에서는 어떤 특정 숫자가 뽑힐 확률은 전부 0이됩니다.
    - 이는 연속된 숫자 사이에서 뽑을 수 있는 숫자의 갯수가 무한하기 때문입니다.
- 따라서 이런 연속사건인 경우 특정 숫자가 나올 확률을 말하는 것은 의미가 없어 다른 방법을 생각해야 하는데, 숫자가 `특정 구간에 속할 확률`을 말하는 것이 그 대안이 될 수 있습니다.
    - 여기서 **특정 구간에 속할 확률**을 구하는 것이 즉, `확률밀도함수(Probability Density Function, PDF)`를 구하는 것 입니다.
- 참고로 위 그림과 같이 `이산적인 구간`에서는 어떤 특정 숫자에 해당하는 확률이 실제 확률을 뜻하고 발생 가능한 숫자들의 확률을 모두 더하면 1이 됩니다.
    - 이와 같이 이산적인 구간에서 어떤 특정 숫자에 해당하는 확률을 `확률질량함수(Probability Mass Function)`이라고 합니다.

<br>

- 앞의 예시의 1에서 6사이의 숫자를 뽑는 상황을 다시 생각해 보겠습니다.
- 1에서 6사이의 숫자 중 정확히 5가 뽑힐 확률은 0이지만 4에서 5사이의 숫자가 뽑힐 확률은 20%입니다.
    - 전체 구간의 길이는 6 - 1 = 5이고 4에서 5사이 구간의 길이는 1이기 때문입니다.
- 이처럼 우리는 특정 사건에 대한 확률 대신 특정 구간에 속할 확률을 구함으로서 간접적으로 특정 사건의 확률에 대한 감을 잡을 수 있습니다.
- 이것을 설명하는 곡선이 **확률밀도함수(Probability Density Function: PDF)** 입니다.
- **PDF**는 특정 구간에 속할 확률을 계산하기 위한 함수이며 **그래프에서 특정 구간에 속한 넓이=특정 구간에 속할 확률**이 되게끔 정한 함수입니다.

<br>
<center><img src="../assets/img/gan/concept/mle_and_map/2.png" alt="Drawing" style="width: 800px;"/></center>
<br>

- 위 그림으로 예를 들어 살펴 보겠습니다. 왼쪽의 그림에서 `PDF`의 값은 1에서 6사이에서는 전부 0.2이고 나머지 구간에서는 전부 0인데, 이는 1에서 6사이의 숫자를 뽑는 상황을 그림으로 나타낸 것입니다.
- 1보다 작거나 6보다 큰 숫자를 뽑을 수는 없으므로 이에 해당하는 확률밀도함수의 함수의 $$ y $$값은 전부 0이고, 1~6사이에서는 무작위로 숫자를 뽑으므로 $$ y $$값은 전부 0.2로 같습니다.
- 전체 확률은 1이므로 그림의 직사각형의 넓이는 1이되고 $$ y $$값은 전부 0.2가 되며, 이를 바탕으로 2에서 4사이의 숫자가 뽑힐 확률을 계산하면 2×0.2=0.42×0.2=0.4로 40%가 됩니다. 
- 오른쪽 그래프는 정규분포(Normal distribution)이며, 가장 흔히 쓰이는 평균 0, 분산 1인 표준정규분포(Standard normal distribution)를 나타냅니다.
- 표준정규분포의 `PDF`는 정규분포식의 구간을 구하면 되는데, 연속 구간에서 영역을 구해야 하므로 적분을 통하여 구할 수 있습니다.
    - 정규분포식 $$ f(x) = \frac{1}{sqrt{2 \pi \sigma^{2}}} exp(\frac{-(x-m)^{2}}{2\sigma^{2}}) $$ 입니다.
- 정규분포의 적분 과정은 다소 복잡하니 표준정규분포 케이스를 통하여 대략적으로 보면 $$ z = (x - m) / \sigma $$의 범위가 -1.96 ~ 1.96이면 95%임이 알려져 있습니다.

<br>

- 정리하면 연속사건의 경우에는 특정 사건이 일어날 확률은 모두 0이며, 어떤 구간에 속할 확률은 `PDF`를 이용해서 구할 수 있습니다.
- 그러면 특정 사건에 대한 해석은 할 수 없는 것일까요? 단순히 할 수 없다 라고 말하기 보다는 다음과 같이 해석해 보는 것도 좋을 것 같습니다.
- 위의 정규분포의 경우를 보면 0이 나올 확률도 0, 1이 나올 확률도 0, 999가 나올 확률도 0으로 모두 같으므로 0, 1, 999가 나올 가능성은 전혀 차이가 없다고 말해야 하지만 정규분포의 그림을 보고 알 수 있드시 표준정규분포에서 가장 위로 솟아올라 있는 0 근처가 나올 가능성이 가장 높고, 1 근처가 나올 가능성은 그보다 낮으며, 999같이 큰 수가 나올 가능성은 거의 없습니다.
- 그러나 **확률이라는 지표로는 이런 연속사건간의 가능성 차이를 표시할 수가 없다는 문제**가 있습니다.

<br>

### **가능도(Likelihood) : 특정 사건이 일어날 가능성**

<br>

- 설명한 대로 **연속사건에서는 특정 사건이 일어날 확률이 전부 0으로 계산**되기 때문에 사건들이 **일어날 가능성을 비교하는 것이 불가능**하며, 가능도라는 개념을 적용해야 이를 비교할 수 있습니다.
- 가능도란 쉽게 말하자면 위에 있는 그래프들에서 $$ y $$값이라고 생각하면 됩니다.
- **$$ y $$값이 높을수록 일어날 가능성이 높은 사건**입니다. 주사위나 동전을 던지는 경우와 같은 **이산확률분포**에서는 $$ y $$값이 각 사건이 일어날 확률을 나타내었으므로 **가능도=확률**이 되어, 확률이 높을수록 일어날 가능성이 높은 사건이 됩니다.
- 반면, 정규분포같이 연속확률분포인 경우는 `PDF`의 값이 바로 $$ y $$가 되며 0에 해당하는 `PDF`값이 0.4로 1에 해당하는 `PDF`값인 0.24보다 높아 0 근처의 숫자가 나올 `가능성`이 1 근처의 숫자가 나올 `가능성`보다 높다고 할 수 있습니다.
- 하지만 0이 나올 `확률`과 1이 나올 `확률`은 모두 0입니다. 이를 정리하면 가능도의 직관적인 정의는 다음과 같습니다.
- 가능도의 직관적인 정의 : 확률분포함수의 $$ y $$ 값
    - 이산확률분포: **가능도 = 확률**
    - 연속확률분포: 가능도 ≠ 확률, **가능도=PDF**

<br>

### **사건이 여러 번 발생할 경우의 가능도**

<br>

- 이번에는 사건이 여러 번 일어날 경우를 생각해 보겠습니다.
- **첫번째 케이스**는 주사위를 3번 던져 각각 1, 3, 6이 나올 확률을 확인하는 문제입니다.
    - 주사위를 던져 1, 3, 6이 나올 확률은 모두 1/6 입니다.
    - 따라서 3번 던져 각각 1, 3, 6이 나올 확률은 (1/6) x (1/6) x (1/6) = 1/216이 됩니다.
- **두번째 케이스**는 동전을 10번 던지는 일을 3회 시행하여 각 시행에서 앞면이 각각 2, 5, 7번 나올 확률을 확인하는 문제입니다.
    - 이항확률 분포를 따르게 되므로 확률은 각각 0.044, 0.246, 0.117입니다. 
    - 따라서 동전을 10번 던져서 각각 2, 5, 7번이 나올 확률은 0.044 x 0.246 x 0.177 = 0.001입니다.
- 가능도도 마찬가지입니다. 앞서 셀 수 있는 사건에서는 확률과 가능도가 같다고 했으므로 주사위를 3번 던져 각각 1, 3, 6 이 나올 가능성을 나타내는 가능도는 1/216입니다.
- 동전을 던지는 경우의 가능도도 마찬가지로 확률과 같은 0.001이 됩니다.

<br>

- 이제 연속사건이 여러 번 일어날 경우를 살펴보자. 앞서 언급한 평균 0, 분산 1인 정규분포에서 숫자를 3번 뽑았을 때 차례대로 -1, 0, 1이 나올 확률은 각각의 사건이 일어날 확률이 모두 0이므로 결국 0이 됩니다.
- 그러나 가능도의 경우 -1, 1이 나올 가능도는 0.24, 0이 나올 가능도는 0.4이므로 -1, 0, 1이 나올 가능도는 0.24 × 0.4 × 0.24 = 0.02가 되어 확률과는 다른 값으로 나타나게 됩니다.

<br>

### **최대우도추정(MLE: Maximum Likelihood Estimation)

<br>

- 앞에서 배운 가능도(Likelihood) 개념을 중심으로 `최대우도추정(MLE: Maximum Likelihood Estimation)`에 대하여 다루어 보려고 합니다.
- 먼저 `MLE`는 random variable의 파라미터를 estimate하는 방법 중 하나인데, 오직 주어진 관측값 또는 데이터를 토대로 파라미터를 estimation 합니다.
    - 예를 들어, p의 확률로 앞면이 나오고 1-p의 확률로 뒷면이 나오는 동전을 던져서 p를 예측한다고 가정해 보겠습니다.
    - MLE로 p를 계산하기 위해서는 간단하게 앞면이 나온 횟수를 전체 횟수로 나누면 됩니다.
- 보다 더 자세한 설명을 위해 알려지지 않은 probability density function $$ f_{0} $$이 있다고 하고 $$ X = (x_{1}, x_{2}, ... , x_{n}) $$를 그 확률로 생성되는 관측값이라고 가정해보겠습니다.
- density function이 $$ \theta $$로 parameterize된 어떤 분포라고 하고 데이터 $$ x $$가 주어진다면, $$ \theta $$의 값을 알 수 있을 때, $$ f(x \vert \theta) $$의 값을 계산할 수 있습니다.
- 만약 $$ f $$가 가우시안 이라면 $$ \theta $$는 평균인 $$ \mu $$와 분산인 $$ \Sigma $$일 것이고, 베르누이라면 $$ 0 \ge p \ge 1 $$가 될 것입니다.
- 이렇게 정의하면 가능도는 다음과 같이 정의할 수 있습니다.

<br>

$$ L(\theta; x_{1}, x_{2}, ... , x_{n}) = L(\theta; X) = f(X \vert \theta) = f(x_{1}, x_{2}, ..., x_{n} \vert \theta) $$

<br>

- Maximum Likelihood Estimation (MLE)는 $$ \theta $$를 estimate하는 방법 중 하나로 Likelihood를 최대로 만드는 값으로 선택하는 것입니다.
- 만약 선택하는 값을 $$ \hat{\theta} $$라고 적는다면, MLE는 다음과 같은 방식으로 값을 찾습니다.

<br>

$$ \hat{\theta} = argmax_{\theta} L(\theta; X) = argmax_{\theta} f(X \vert \theta) $$

<br>
       
- 만약 관측값들이 `i.i.d(independent and identical distributed)`라면 $$ f(X \vert \theta) = \prod_{i}f(x_{i} \vert \theta) $$가 되며, 여기에 $$ log $$를 씌우면 덧셈 꼴이 됩니다.
- 이 때, $$ log $$는 단조증가함수이므로, $$ log $$를 취했을 때 최대값을 가지는 지점과 원래 최대값을 가지는 지점이 동일하고 곱셈보다 덧셈이 계산이 더 간편하므로 많은 경우에 **likelihood**를 사용하기 보다는 **log likelihood**를 사용하여 파라미터 estimation을 계산합니다.
- `MLE`는 가장 간단한 파라미터 estimation 방법이지만, 관측값에 따라 그 값이 너무 민감하게 변한다는 단점이 있습니다. 
    - 예를 들어 동전 던지기의 극단적인 경우로 n번 던져서 n번 앞면이 나오는 경우 likelihood를 1이라고 하면 합리적일까요? 




<br>

### **MLE 예시**

<br>






<br>

### **최대 사후 확률 추정(MAP: Maximum A Posterior)**

<br>






<br>