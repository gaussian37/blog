---
layout: post
title: 경량 딥러닝 기술 동향(ETRI)
date: 2020-01-03 00:00:00
img: dl/concept/deep-neural-network.jpg
categories: [dl-concept]
tags: [lightweight, deep learning, 경량 딥러닝] # add tag
---

<br>

- 출처 : https://ettrends.etri.re.kr
- 이 글을 경량 딥러닝 기술 동향에 대한 ETRI의 글 중에 필요한 내용만 발췌한 글입니다. 내용이 참 좋으니 전문을 읽어보시는 것 또한 추천드립니다.

<br>

## **경량 딥러닝의 필요성**

<br>

- 경량 디바이스, 모바일 디바이스 및 다양한 센서와 같은 디바이스에서 직접 학습과 추론을 하려면 기존의 PC 레벨의 성능과 GPU를 사용하기에는 무리가 있습니다.
- 따라서 기존의 학습된 모델의 정확도를 유지하면서 보다 크기가 작고, 연산을 간소화하는 경량 딥러닝이 필수적입니다.
- 경량 딥러닝 기술은 크게 **적은 연산과 효율적인 구조**로 설계하여 효율을 극대화 시키는 방법과 **모델의 파라미터를 줄이는 방법**을 적용한 모델 압축 방법등으로 나눌 수 있습니다.

<br>

## **경량 딥러닝 관련**

<br>

- CNN계열의 모델에서 주로 학습 시 가장 큰 연상량을 필요로 하는 합성곱 연산을 줄이기 위하여 효율적인 합성곱 필터 방법들이 연구가 되고 일반화 되었습니다.
- 주로 다루어졌던 것이 기본 단일 층 별 연산에 그치지 않고 연산량과 파라미터의 수를 줄이기 위한 `Residual Block` 또는 `Bottleneck Block`과 같은 형태를 반복적으로 쌓아 신경망을 구성하는 방법입니다.
- 또는 기존 신경망의 모델 구조를 인간이 찾지 않고 모델 구조를 자동 탐색하여 모델을 자동화하거나 연산량 대비 모델 압축 비율을 조정하는 등 자동 탐색 기술등도 연구되었습니다.
  - 이는 모바일 딥러닝과 같은 다양한 기기의 성능 대비 추론 속도가 중요한 어플리케이션을 위해 정확도, 지연시간, 전력량 등을 고려하여 강화 학습을 사용하여 경량 모델을 탐색하는 방법입니다.

<br>

- 또한 모델이 가지는 파라미터의 크기를 줄이는 방법도 다양하게 연구되어 왔습니다.
- 일반적으로 크기가 큰 딥러닝 모델은 파라미터가 과하게 많습니다. 어떤 파라미터의 값의 크기가 작을 경우 모델의 정확도에 큰 영향을 미치지 못하므로 무시하는 방법을 취하기도 하는 데 이것을 `pruning` 이라고 합니다.
- 즉 파라미터의 값에 0을 대입하여 모델이 작은 가중치에 대하여 내성이 생기도록 하는데 파라미터 (weight)에 가지치기를 하므로 `weight pruning` 이라고도 합니다.
- 또한 일반적인 모델의 가중치는 부동 소수점을 가지지만 이를 특정 비트수로 줄이는 `Quantization`을 통해 기존 딥러닝의 표현력은 유지하면서 실제 모델의 저장 크기는 줄이는 방법이 있습니다.
- 더 나아가 0과 1로 파라미터 값을 표현하여 표현력은 많이 줄지만 정확도와 손실을 어느 정도 유지하면서 모델 저장 크기를 확연히 줄이는 `Binarization`에 대한 기법도 연구되었습니다.
- 따라서 **경량 딥러닝은 딥러닝 모델의 구조적 한계를 극복하고자 하는 경량 딥러닝 알고리즘과 기존 모델의 효율적인 사용을 위한 알고리즘 경량화 두 축으로** 연구가 진행중입니다.

<br>
<center><img src="../assets/img/dl/concept/light_weight_dl/0.png" alt="Drawing" style="width: 800px;"/></center>
<br>
