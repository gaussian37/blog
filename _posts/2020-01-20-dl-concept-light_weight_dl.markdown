---
layout: post
title: 경량 딥러닝 기술 동향(ETRI)
date: 2020-01-03 00:00:00
img: dl/concept/deep-neural-network.jpg
categories: [dl-concept]
tags: [lightweight, deep learning, 경량 딥러닝] # add tag
---

<br>

- 출처 : https://ettrends.etri.re.kr
- 이 글을 경량 딥러닝 기술 동향에 대한 ETRI의 글 중에 필요한 내용만 발췌한 글입니다. 내용이 참 좋으니 전문을 읽어보시는 것 또한 추천드립니다.

<br>

## **경량 딥러닝의 필요성**

<br>

- 경량 디바이스, 모바일 디바이스 및 다양한 센서와 같은 디바이스에서 직접 학습과 추론을 하려면 기존의 PC 레벨의 성능과 GPU를 사용하기에는 무리가 있습니다.
- 따라서 기존의 학습된 모델의 정확도를 유지하면서 보다 크기가 작고, 연산을 간소화하는 경량 딥러닝이 필수적입니다.
- 경량 딥러닝 기술은 크게 **적은 연산과 효율적인 구조**로 설계하여 효율을 극대화 시키는 방법과 **모델의 파라미터를 줄이는 방법**을 적용한 모델 압축 방법등으로 나눌 수 있습니다.

<br>

## **경량 딥러닝 관련**

<br>

- CNN계열의 모델에서 주로 학습 시 가장 큰 연상량을 필요로 하는 합성곱 연산을 줄이기 위하여 효율적인 합성곱 필터 방법들이 연구가 되고 일반화 되었습니다.
- 주로 다루어졌던 것이 기본 단일 층 별 연산에 그치지 않고 연산량과 파라미터의 수를 줄이기 위한 `Residual Block` 또는 `Bottleneck Block`과 같은 형태를 반복적으로 쌓아 신경망을 구성하는 방법입니다.
- 또는 기존 신경망의 모델 구조를 인간이 찾지 않고 모델 구조를 자동 탐색하여 모델을 자동화하거나 연산량 대비 모델 압축 비율을 조정하는 등 자동 탐색 기술등도 연구되었습니다.
  - 이는 모바일 딥러닝과 같은 다양한 기기의 성능 대비 추론 속도가 중요한 어플리케이션을 위해 정확도, 지연시간, 전력량 등을 고려하여 강화 학습을 사용하여 경량 모델을 탐색하는 방법입니다.

<br>

- 또한 모델이 가지는 파라미터의 크기를 줄이는 방법도 다양하게 연구되어 왔습니다.
- 일반적으로 크기가 큰 딥러닝 모델은 파라미터가 과하게 많습니다. 어떤 파라미터의 값의 크기가 작을 경우 모델의 정확도에 큰 영향을 미치지 못하므로 무시하는 방법을 취하기도 하는 데 이것을 `pruning` 이라고 합니다.
- 즉 파라미터의 값에 0을 대입하여 모델이 작은 가중치에 대하여 내성이 생기도록 하는데 파라미터 (weight)에 가지치기를 하므로 `weight pruning` 이라고도 합니다.
- 또한 일반적인 모델의 가중치는 부동 소수점을 가지지만 이를 특정 비트수로 줄이는 `Quantization`을 통해 기존 딥러닝의 표현력은 유지하면서 실제 모델의 저장 크기는 줄이는 방법이 있습니다.
    - `Quantization` : 부동 소수점을 특정 비트(e.g. 8bit)로 구간을 나누어서 표현하는 방법으로 부동 소수점 대비 그 크기를 줄일 수 있습니다.
- 더 나아가 0과 1로 파라미터 값을 표현하여 표현력은 많이 줄지만 정확도와 손실을 어느 정도 유지하면서 모델 저장 크기를 확연히 줄이는 `Binarization`에 대한 기법도 연구되었습니다.
- 따라서 **경량 딥러닝은 딥러닝 모델의 구조적 한계를 극복하고자 하는 경량 딥러닝 알고리즘과 기존 모델의 효율적인 사용을 위한 알고리즘 경량화 두 축으로** 연구가 진행중입니다.

<br>
<center><img src="../assets/img/dl/concept/light_weight_dl/0.png" alt="Drawing" style="width: 800px;"/></center>
<br>

## **모델 구조 변경 기술**

<br>

- CNN 알고리즘 초기에는 convolution 연산 이후 다운 샘플링을 통해 통과하는 격자의 크기를 줄여 연산량과 변수가 많아 학습되지 않는 문제를 해결하고자 하였습니다.
- 점차적으로 필터의 크기가 줄어들면서 1x1 필터의 사용이 중요해지게 되었고 더 나아가 필터 축소 이외에 서로 다른 필터를 `병렬`로 연결하는 인셉션 모듈을 통하여 다양한 형태로 발전하게 되었습니다.
- 필터를 병렬로 연결하는 것이 더 발전하여 `ResNet`과 같이 두 개의 연속적인 합성곱 층에 단위행렬의 추가를 위한 지름길을 더해 줌으로써 가중치들이 더 쉽게 최적화될 수 있는 `Residual Block` 형태로 개선되었으며, 이것을 기반으로 `Bottleneck Architecture` 또는 `Dense Block` 형태로 발전되었습니다.

<br>

### **ResNet**

<br>
<center><img src="../assets/img/dl/concept/light_weight_dl/1.png" alt="Drawing" style="width: 800px;"/></center>
<br>

- 위 그림과 같이 레어의 수가 늘어날수록 점차 정확도가 저하되는 문제가 발생하는데 shortcut을 통해 파라미터 없이 바로 연결되는 구조로 바꾸고, 연산량 관점에서 덧셈이 추가되는 형태로 문제를 단순화 할 수 있었습니다.
- 이 시점 이후에 Residual Block을 사용하는 다양한 네트워크가 생기게 되었고 깊은 신경망에서도 최적화가 가능해졌고 정확도의 개선 효과도 있었습니다.

<br>

### **DenseNet**

<br>

- 기존 신경망 모델 구조의 여러 장점을 모아서 DenseNet이 소개 되었습니다. DenseNet에서는 기존 Feature map에 덧셈 연산을 통해 합치는 것이 아니라 **concat 해서 쌓아가는 과정**을 통해 성능을 높이고자 하였습니다.
- 또한 이전에는 가장 마지막 레이어에서 추출한 정보를 이용하여 문제를 해결하였는데 (e.g. classification) DenseNet에서는 이전의 모든 층에서의 정보를 취득하는 형태가 가능하게 되었습니다. 이를 통해, 기존의 다른 네트워크보다 좁게 설계가 가능해 지고 **파라미터의 수를 줄일 수 **있게 되었습니다.


## **효율적인 합성곱 필터 기술**

<br>

## **경량 모델 자동 탐색 기술**

<br>

## **모델 압축 기술**

<br>

## **Knowledge Distillation**

<br>

